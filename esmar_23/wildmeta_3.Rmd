---
title: "wildmeta  0.3.0: Cluster Wild Bootstrapping for Meta-Analysis"
subtitle: "ESMAR 2023"
author: "Megha Joshi, James E. Pustejovsky & Pierce Cappelli"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      after_body: insert_logo.html
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(xaringanExtra)

style_mono_accent(
  base_color = "#311432",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)

library(tidyverse)
library(knitr)
library(kableExtra)
library(wildmeta)
library(future)
library(metafor)
library(clubSandwich)
```


# Dependence 

- Typical meta-analytic techniques (like meta-regression) involves the assumption that effect sizes are independent 


- However, common for each primary study to yield more than one effect size or studies to be nested in some way creating dependence 

- Example: Kalaian and Raudenbush (1996) data from `clubSandwich` with effects of SAT (US standardized test for college admissions) coaching 
  - 47 studies, 67 effect sizes
  - Effects in same study from math and verbal sections of the test
  
---

# Data `SATcoaching`

```{r, warning = F, message = F, echo = F}
library(clubSandwich)
library(DT)

DT::datatable(SATcoaching %>% select(study, d, V, test, hrs))
```


---

# Handling Dependence

- Ignoring dependence leads to incorrect standard errors, incorrect inference from hypothesis tests

- Robust variance estimation (RVE) with small sample correction recommended (Tipton, 2015; Tipton and Pustejovsky, 2015)
  
- Such correction controls Type 1 error rates adequately but low power especially for __multiple-contrast hypothesis tests__ (Joshi, Pustejovsky and Beretvas, 2022; Tipton and Pustejovsky, 2015)
  - Examples of multiple contrast hypothesis test: To what extent do the effects of SAT coaching vary across math and verbal sections of the test?
  
---

# Cluster Wild Bootstrapping

- We examined an alternative method cluster wild bootstrapping (CWB) (Joshi, Pustejovsky and Beretvas, 2022)

- Bootstrapping involves estimating unknown quantities by re-sampling from original data many times (Boos et al., 2013)
  
- CWB involves re-sampling residuals by multiplying them by random cluster-level weights (Cameron, Gelbach, and Miller 2008)

- CWB maintains adequate Type 1 error rates and has more power than RVE small-sample correction (Joshi, Pustejovsky and Beretvas, 2022)

---

# CWB Algorithm

1. Fit a null model and a full model on the original data

2. Obtain residuals from the null model 

3. Generate an auxiliary random variable that has mean of 0 and variance of 1 and multiply the residuals by the random variable (e.g., Rademacher weights) set to be constant within clusters (CWB)
  - Can also multiply the residuals by CR2 matrices before multiplying by weights (CWB Adjusted)

4.  Obtain new outcome scores by adding the transformed residuals to the predicted values from the null model fit on the original data

5.  Re-estimate the full model with the new calculated outcome scores and obtain the test statistic

6. Repeat steps 3-5 $R$ times. Calculate p-value:

$$p = \frac{1}{R} \sum_{r = 1}^R I\left(F^{(r)} > F\right)$$


---

# wildmeta

- The main function in the package is `Wald_test_cwb()`

- Works with meta-regressions models fit using `robumeta::robu()`, `metafor::rma.mv()` and `metafor::rma.uni()`

```{r, eval = F}
Wald_test_cwb(
  full_model,
  constraints,
  R,
  cluster = NULL,
  auxiliary_dist = "Rademacher",
  adjust = "CR0",
  type = "CR0",
  test = "Naive-F",
  seed = NULL
)
```

---

# Data `SATcoaching`

```{r, warning = F, message = F, echo = F}
library(clubSandwich)
library(DT)

DT::datatable(SATcoaching %>% select(study, d, V, test, hrs))
```


---

# robumeta Model

```{r, warning= FALSE, message = F}
library(wildmeta)
library(clubSandwich)
library(robumeta)

robu_model <- robu(d ~ 0 + test + hrs,
                   studynum = study,
                   var.eff.size = V,
                   small = FALSE,
                   data = SATcoaching)


```

---

# `robumeta` Results

```{r}
robu_model
```


---

# robumeta

```{r, message = F, warning = F, eval = F}
robu_res <- Wald_test_cwb(full_model = robu_model,
                          constraints = constrain_equal(1:2),
                          R = 999,
                          seed = 20220209)

robu_res
```

```{r, echo = F}
#save(robu_res, file = "robu_res.RData")
load("robu_res.RData")
robu_res
```


---

# metafor

```{r, warning=F, message=F, eval = F}
library(metafor)

rma_model <- rma.mv(yi = d ~ 0 + test + hrs,
                    V = V,
                    random = ~ test | study,
                    data = SATcoaching)

rma_res <- Wald_test_cwb(full_model = rma_model,
                         constraints = constrain_equal(1:2),
                         R = 999,
                         seed = 20210314)

rma_res
```

```{r, echo = F}
#save(rma_res, file = "rma_res.RData")
load("rma_res.RData")
rma_res
```

---

# Parallel Processing

```{r}
library(future)

if (parallelly::supportsMulticore()) {
  plan(multicore) 
} else {
  plan(multisession)
}

nbrOfWorkers()

system.time(
  robu_res_para <- Wald_test_cwb(full_model = robu_model,
                                 constraints = constrain_equal(1:2),
                                 R = 1999, 
                                 seed = 20230202)
)
```

---

# `robumeta` Parallel

```{r}
robu_res_para
```


---

# How Many Bootstraps?

- Higher number of bootstraps better ~ precision, power

- Computationally intensive

- 1,999 or higher

- See [Davidson & MacKinnon (2000)](https://www.econstor.eu/bitstream/10419/67820/1/587473266.pdf) for guidance on number of bootstraps

---

class: middle, center, cobBack, hide-logo

# Links

wildmeta: [https://meghapsimatrix.github.io/wildmeta/index.html](https://meghapsimatrix.github.io/wildmeta/index.html) 

My website: [https://meghapsimatrix.com](https://meghapsimatrix.com)


---

class: hide-logo

```{r echo = FALSE, out.height = 500, out.width = 650, fig.align = "center"}
knitr::include_graphics("wildmeta_hex.png")
```


---

class: inverse, middle, center, cobBack, hide-logo

# THANK YOU!!

---


class: inverse, hide-logo

# References

Boos, D. D., & others. (2003). Introduction to the bootstrap world. Statistical Science, 18(2), 168–174. 

Cameron, A. C., Gelbach, J. B., & Miller, D. L. (2008). Bootstrap-Based Improvements for Inference with Clustered Errors. The Review of Economics and Statistics, 47.

Davidson, R., & MacKinnon, J. G. (2000). Bootstrap tests: How many bootstraps?. Econometric Reviews, 19(1), 55-68.

Hedges, L. V., Tipton, E., & Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. Research Synthesis Methods, 1(1), 39–65.

Joshi, M., Pustejovsky, J. E., & Beretvas, S. N. (2022). Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with small number of studies. Research Synthesis Methods. Research Synthesis Methods.

---

class: inverse, hide-logo

# References

Kalaian, H. A. & Raudenbush, S. W. (1996). A multivariate mixed linear model for meta-analysis. Psychological Methods, 1(3), 227-235. doi: 10.1037/1082-989X.1.3.227

Tipton, E., & Pustejovsky, J. E. (2015). Small-Sample Adjustments for Tests of Moderators and Model Fit Using Robust Variance Estimation in Meta-Regression. Journal of Educational and Behavioral Statistics, 40 (6), 604–634. 

Tipton, E. (2015). Small sample adjustments for robust variance estimation with meta-regression. Psychological Methods, 20(3), 375–393. 